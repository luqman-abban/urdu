# -*- coding: utf-8 -*-
"""Copy of Fine tuning wav2vec with Mozilla Urdu CV .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iC2Dm545sgpn8v4nwBb8sApYDI7fanUV
"""

!pip install datasets
!pip install transformers
!pip install jiwer
!pip install torchaudio
!pip install librosa
!pip install git-lfs

from huggingface_hub import notebook_login

notebook_login()

from datasets import load_dataset

train_dataset = load_dataset("mozilla-foundation/common_voice_13_0", "ur", split="train[:10%]")

print(train_dataset)

validation_dataset = load_dataset("mozilla-foundation/common_voice_13_0", "ur", split="validation[:10%]")
test_dataset = load_dataset("mozilla-foundation/common_voice_13_0", "ur", split="test[:10%]")

print(validation_dataset)
print(test_dataset)

train_dataset  = train_dataset.remove_columns(["client_id","path", "up_votes", "down_votes", "age","gender", "accent", "locale", "segment","variant"])
test_dataset  = test_dataset.remove_columns(["client_id","path", "up_votes", "down_votes", "age", "gender", "accent","locale", "segment","variant"])
validation_dataset = validation_dataset.remove_columns(["client_id","path", "up_votes", "down_votes", "age", "gender", "accent","locale", "segment","variant"])

import pandas as pd
display(pd.DataFrame(train_dataset[:5]))

display(pd.DataFrame(test_dataset[:5]))

import re

chars_to_ignore_regex = '[\,\?\.\!\-\;\:\"\“\%\‘\”\�\']'

def remove_special_characters(batch):
    if 'sentence' in batch:
        batch['sentence'] = re.sub(chars_to_ignore_regex, '', batch['sentence']).lower() + " "
    return batch

train_dataset  = train_dataset.map(remove_special_characters)
test_dataset  = test_dataset.map(remove_special_characters)
validation_dataset = validation_dataset.map(remove_special_characters)

def extract_all_chars(batch):
    all_text = " ".join(batch["sentence"])
    vocab = list(set(all_text))
    return {"vocab": [vocab], "all_text": [all_text]}

def extract_all_chars(examples):
    vocabs = []
    for text in examples["sentence"]:
        vocab = list(set(text))  # extract unique characters from the text
        vocabs.append(vocab)
    return {"vocab": vocabs}

print(train_dataset[0]["sentence"])

vocab_train = train_dataset.map(extract_all_chars, batched=True, batch_size=-1, remove_columns=train_dataset.column_names)

from collections import Counter

def extract_all_chars(examples):
    vocabs = []
    for text in examples["sentence"]:
        vocab = list(set(text))
        vocabs.append(vocab)
    return {"vocab": vocabs}

# 6. Extract vocabulary from the training set
vocab_train = train_dataset.map(extract_all_chars, batched=True, batch_size=-1, remove_columns=train_dataset.column_names)

# 7. Create vocabulary list and dictionary
vocab_list = [item for sublist in vocab_train["vocab"] for item in sublist]

# 7.1. Count character occurrences
char_counts = Counter(vocab_list)

# 7.2. Set a frequency threshold (e.g., characters must appear at least 5 times)
threshold = 5

# 7.3. Filter out rare characters
filtered_vocab_list = [char for char in vocab_list if char_counts[char] >= threshold]

# 7.4. Create the vocabulary dictionary from the filtered list
vocab_dict = {v: k for k, v in enumerate(sorted(set(filtered_vocab_list)))}

# 8. Add special tokens
vocab_dict["[UNK]"] = len(vocab_dict)
vocab_dict["[PAD]"] = len(vocab_dict)

# 9. Save the vocabulary
with open("vocab.json", "w", encoding="utf-8") as vocab_file:
    json.dump(vocab_dict, vocab_file, ensure_ascii=False, indent=4)

vocab_dict

len(vocab_dict)

from transformers import Wav2Vec2CTCTokenizer

tokenizer = Wav2Vec2CTCTokenizer("./vocab.json", unk_token="[UNK]", pad_token="[PAD]", word_delimiter_token=" ")

test_sentence = "یہ ایک مثال ہے۔"
encoded = tokenizer(test_sentence)
decoded = tokenizer.decode(encoded["input_ids"])

print("Encoded IDs:", encoded["input_ids"])
print("Decoded Sentence:", decoded)

from transformers import Wav2Vec2FeatureExtractor

feature_extractor = Wav2Vec2FeatureExtractor(feature_size=1, sampling_rate=16000, padding_value=0.0, do_normalize=True, return_attention_mask=True)

from transformers import Wav2Vec2Processor

processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)

import torchaudio
def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch["audio"]["path"])
    batch["speech"] = speech_array[0].numpy()
    batch["sampling_rate"] = sampling_rate
    batch["target_text"] = batch["sentence"]
    return batch

train_dataset  = train_dataset.map(speech_file_to_array_fn, remove_columns=train_dataset.column_names)
test_dataset  = test_dataset.map(speech_file_to_array_fn, remove_columns=test_dataset.column_names)
validation_dataset = validation_dataset.map(speech_file_to_array_fn, remove_columns=validation_dataset.column_names)

import librosa
import numpy as np

def resample(batch):
    # Ensure "speech" and "sampling_rate" are in the batch
    if "speech" in batch and "sampling_rate" in batch:
        original_sampling_rate = batch["sampling_rate"]
        target_sampling_rate = 16000  # Change as needed
        # Perform resampling
        batch["speech"] = librosa.resample(
            np.asarray(batch["speech"]),
            orig_sr=original_sampling_rate,
            target_sr=target_sampling_rate
        )
        batch["sampling_rate"] = target_sampling_rate  # Update the sampling rate
    return batch

train_dataset = train_dataset.map(resample)
test_dataset = test_dataset.map(resample)
validation_dataset = validation_dataset.map(resample)

import IPython.display as ipd
import numpy as np
import random

rand_int = random.randint(0, len(train_dataset)-1)

ipd.Audio(data=np.asarray(train_dataset[rand_int]["speech"]), autoplay=True, rate=16000)

print("Target text:", train_dataset[rand_int]["target_text"])
print("Input array shape:", np.asarray(train_dataset[rand_int]["speech"]).shape)
print("Sampling rate:", train_dataset[rand_int]["sampling_rate"])

def prepare_dataset(batch):
    # check that all files have the correct sampling rate
    assert (
        len(set(batch["sampling_rate"])) == 1
    ), f"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}."

    batch["input_values"] = processor(batch["speech"], sampling_rate=batch["sampling_rate"][0]).input_values

    with processor.as_target_processor():
        batch["labels"] = processor(batch["target_text"]).input_ids
    return batch

train_dataset = train_dataset.map(prepare_dataset, remove_columns=train_dataset.column_names, batch_size=8, num_proc=4, batched=True)
test_dataset = test_dataset.map(prepare_dataset, remove_columns=test_dataset.column_names, batch_size=8, num_proc=4, batched=True)
validation_dataset = validation_dataset.map(prepare_dataset, remove_columns=validation_dataset.column_names, batch_size=8, num_proc=4, batched=True)

train_dataset

import torch

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Union

@dataclass
class DataCollatorCTCWithPadding:

    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Optional[int] = None
    max_length_labels: Optional[int] = None
    pad_to_multiple_of: Optional[int] = None
    pad_to_multiple_of_labels: Optional[int] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # split inputs and labels since they have to be of different lenghts and need
        # different padding methods
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )
        with self.processor.as_target_processor():
            labels_batch = self.processor.pad(
                label_features,
                padding=self.padding,
                max_length=self.max_length_labels,
                pad_to_multiple_of=self.pad_to_multiple_of_labels,
                return_tensors="pt",
            )

        # replace padding with -100 to ignore loss correctly
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch

data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)

def compute_metrics(pred):
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.batch_decode(pred_ids)
    # we do not want to group tokens when computing the metrics
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer}

!pip install evaluate

import evaluate
wer_metric = evaluate.load("wer")

import os
os.environ['WANDB_DISABLED'] = 'true'

from transformers import Wav2Vec2ForCTC

model = Wav2Vec2ForCTC.from_pretrained(
    "facebook/wav2vec2-large-xlsr-53",
    attention_dropout=0.1,
    hidden_dropout=0.1,
    feat_proj_dropout=0.0,
    mask_time_prob=0.05,
    layerdrop=0.1,
    gradient_checkpointing=True,
    ctc_loss_reduction="mean",
    pad_token_id=processor.tokenizer.pad_token_id,
    vocab_size=len(processor.tokenizer)
)

model.freeze_feature_extractor()

from transformers import TrainingArguments, EarlyStoppingCallback
from google.colab import drive
drive.mount('/content/drive')

training_args = TrainingArguments(
    output_dir="/content/drive/My Drive/urdu_trained_model",
    per_device_train_batch_size=16,
    gradient_accumulation_steps=4,
    evaluation_strategy="epoch",
    num_train_epochs=30,
    fp16=True,
    save_steps=500,
    eval_steps=500,
    logging_steps=100,
    learning_rate=5e-5,
    warmup_steps=1000,
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model="wer",
    greater_is_better=False,
    save_strategy="epoch",
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    data_collator=data_collator,
    args=training_args,
    compute_metrics=compute_metrics,
    train_dataset=train_dataset,
    eval_dataset=validation_dataset,
    tokenizer=processor.feature_extractor
     #callbacks=[early_stopping_callback]
    )

trainer.train()

import torch

# Load the fine-tuned model
model_path = "/content/drive/My Drive/urdu_trained_model"
model = Wav2Vec2ForCTC.from_pretrained(model_path)

# Load the processor
processor = Wav2Vec2Processor.from_pretrained(model_path)

# Function to perform inference on a single audio file
def predict(audio_file):
    speech_array, sampling_rate = torchaudio.load(audio_file)
    speech_array = speech_array[0].numpy()
    speech_array = librosa.resample(np.asarray(speech_array), orig_sr=sampling_rate, target_sr=16000)

    input_values = processor(speech_array, sampling_rate=16000, return_tensors="pt").input_values
    logits = model(input_values).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.batch_decode(predicted_ids)[0]
    return transcription

# Test the model on the test dataset
for i in range(len(test_dataset)):
    audio_path = test_dataset[i]["audio"]["path"]
    print(f"Sample {i+1}:")
    print("Predicted Transcription:", transcription)
    print("Actual Transcription:", test_dataset[i]["sentence"])

from huggingface_hub import HfApi, Repository

# 1. Create a Hugging Face repo
repo_id = "your-username/urdu-speech-recognition" #Replace with your desired repo name
api = HfApi()
repo_url = api.create_repo(repo_id=repo_id, exist_ok=True)


# 2. Push the model and processor to the repo
model.push_to_hub(repo_id)
processor.push_to_hub(repo_id)

# 3. Create or update the Hugging Face Space
# You'll need to create a Space manually on Hugging Face and add the deployment files,
# including the inference code and an app.py or gradio script.

# Example app.py (you'll need to adapt this based on your Space requirements)
#
# from huggingface_hub import hf_hub_download
# import gradio as gr
# import torch
# from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
#
# # Load the model and processor from Hugging Face Hub
# model_id = "your-username/urdu-speech-recognition"  # Replace with your repo ID
# model = Wav2Vec2ForCTC.from_pretrained(model_id)
# processor = Wav2Vec2Processor.from_pretrained(model_id)

# ... (rest of your Space deployment code)